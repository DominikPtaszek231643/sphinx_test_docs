{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "\n",
    "def load_and_display_video(video_path):\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        sys.exit()\n",
    "    \n",
    "    # Read and display frame by frame\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Reached the end of the video or an error occurred. Exiting.\")\n",
    "            break\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow('Video Frame', frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # When everything done, release the capture and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Replace 'path_to_your_video.mp4' with the path to the video file you want to load\n",
    "load_and_display_video('path_to_your_video.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_frame(frame, target_size=(224, 224), normalization=True):\n",
    "    \"\"\"\n",
    "    Pre-process a single frame:\n",
    "    - Resize frame\n",
    "    - Normalize pixel values\n",
    "    \"\"\"\n",
    "    # Resize the frame\n",
    "    resized_frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    if normalization:\n",
    "        # Normalize the frame's pixel values to be between 0 and 1\n",
    "        normalized_frame = resized_frame / 255.0\n",
    "        return normalized_frame\n",
    "    else:\n",
    "        return resized_frame\n",
    "\n",
    "def select_key_frames(video_path, skip_frames=5):\n",
    "    \"\"\"\n",
    "    Load video, and pre-process video frames, selecting key frames based on a skip value.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return []\n",
    "    \n",
    "    processed_frames = []\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Select key frames based on skip_frames\n",
    "        if frame_count % skip_frames == 0:\n",
    "            processed_frame = preprocess_frame(frame)\n",
    "            processed_frames.append(processed_frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(processed_frames)\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'path_to_your_video.mp4' with the actual video file path\n",
    "video_frames = select_key_frames('path_to_your_video.mp4', skip_frames=5)\n",
    "print(f\"Processed {len(video_frames)} frames.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fe21e2bb5b7b81b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load MobileNetV2 with pre-trained ImageNet weights, excluding the top (classification) layer\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Define a model that will return the output of the base_model's last layer\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "def extract_features(video_frames):\n",
    "    \"\"\"\n",
    "    Extract features for each frame in video_frames using MobileNetV2.\n",
    "    \"\"\"\n",
    "    # Assuming video_frames is a 4D numpy array: (num_frames, height, width, channels)\n",
    "    features = model.predict(video_frames)\n",
    "    return features\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d6cea8ed98ff9c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Assuming we're working with a fixed feature size from MobileNetV2\n",
    "feature_size = 7 * 7 * 1280  # This will depend on the output shape of your CNN\n",
    "num_classes = 10  # Example number of classes for classification\n",
    "\n",
    "# Define a simple LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(256, input_shape=(None, feature_size)),  # Adjust based on the shape of the CNN output\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Assuming `extracted_features` is a numpy array of shape (num_samples, num_frames, feature_size)\n",
    "# and `labels` is a one-hot encoded numpy array of shape (num_samples, num_classes)\n",
    "# extracted_features = extract_features(preprocessed_video_frames)  # From previous step\n",
    "# labels = ...  # Load or define your labels here\n",
    "\n",
    "# Fit the model\n",
    "# lstm_model.fit(ex\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9585bfb14f61d01e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
